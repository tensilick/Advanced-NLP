
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D Assignment #2\n",
    "# Part [1]: Deep Networks: NER Window Model\n",
    "\n",
    "For this first part of the assignment, you'll build your first \"deep\" networks. On problem set 1, you computed the backpropagation gradient $\\frac{\\partial J}{\\partial w}$ for a two-layer network; in this problem set you'll implement a slightly more complex network to perform  named entity recognition (NER).\n",
    "\n",
    "Before beginning the programming section, you should complete parts (a) and (b) of the corresponding section of the handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c): Random Initialization Test\n",
    "Use the cell below to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46994114 -0.83008197  0.23148553  0.43094097 -0.00258593]\n",
      " [-0.47666619 -0.52297046  0.45125243 -0.57311684 -0.71301636]\n",
      " [ 0.32105262  0.78530031 -0.85918681  0.02111762  0.54147539]]\n"
     ]
    }
   ],
   "source": [
    "from misc import random_weight_matrix\n",
    "random.seed(10)\n",
    "print random_weight_matrix(3,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d): Implementation\n",
    "\n",
    "We've provided starter code to load in the dataset and convert it to a list of \"windows\", consisting of indices into the matrix of word vectors. \n",
    "\n",
    "We pad each sentence with begin and end tokens `<s>` and `</s>`, which have their own word vector representations; additionally, we convert all words to lowercase, canonicalize digits (e.g. `1.12` becomes `DG.DGDG`), and replace unknown words with a special token `UUUNKKK`.\n",
    "\n",
    "You don't need to worry about the details of this, but you can inspect the `docs` variables or look at the raw data (in plaintext) in the `./data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import data_utils.utils as du\n",
    "import data_utils.ner as ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the starter word vectors\n",
    "wv, word_to_num, num_to_word = ner.load_wv('data/ner/vocab.txt',\n",
    "                                           'data/ner/wordVectors.txt')\n",
    "tagnames = [\"O\", \"LOC\", \"MISC\", \"ORG\", \"PER\"]\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = du.invert_dict(num_to_tag)\n",
    "\n",
    "# Set window size\n",
    "windowsize = 3\n",
    "\n",
    "# Load the training set\n",
    "docs = du.load_dataset('data/ner/train')\n",
    "X_train, y_train = du.docs_to_windows(docs, word_to_num, tag_to_num,\n",
    "                                      wsize=windowsize)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/ner/dev')\n",
    "X_dev, y_dev = du.docs_to_windows(docs, word_to_num, tag_to_num,\n",
    "                                  wsize=windowsize)\n",
    "\n",
    "# Load the test set (dummy labels only)\n",
    "docs = du.load_dataset('data/ner/test.masked')\n",
    "X_test, y_test = du.docs_to_windows(docs, word_to_num, tag_to_num,\n",
    "                                    wsize=windowsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid re-inventing the wheel, we provide a base class that handles a lot of the drudgery of managing parameters and running gradient descent. It's based on the classifier API used by [`scikit-learn`](http://scikit-learn.org/stable/), so if you're familiar with that library it should be easy to use. \n",
    "\n",
    "We'll be using this class for the rest of this assignment, so it helps to get acquainted with a simple example that should be familiar from Assignment 1. To keep this notebook uncluttered, we've put the code in the `softmax_example.py`; take a look at it there, then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/db error norm = 3.565e-10 [ok]\n",
      "    b dims: [5] = 5 elem\n",
      "grad_check: dJ/dW error norm = 2.164e-11 [ok]\n",
      "    W dims: [5, 100] = 500 elem\n",
      "grad_check: dJ/dL[5] error norm = 2.646e-11 [ok]\n",
      "    L[5] dims: [100] = 100 elem\n"
     ]
    }
   ],
   "source": [
    "from softmax_example import SoftmaxRegression\n",
    "sr = SoftmaxRegression(wv=zeros((10,100)), dims=(100,5))\n",
    "\n",
    "##\n",
    "# Automatic gradient checker!\n",
    "# this checks anything you add to self.grads or self.sgrads\n",
    "# using the method of Assignment 1\n",
    "sr.grad_check(x=5, y=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement a model, you need to subclass `NNBase`, then implement the following methods:\n",
    "\n",
    "- `__init__()` (initialize parameters and hyperparameters)\n",
    "- `_acc_grads()` (compute and accumulate gradients)\n",
    "- `compute_loss()` (compute loss for a training example)\n",
    "- `predict()`, `predict_proba()`, or other prediction method (for evaluation)\n",
    "\n",
    "`NNBase` provides you with a few others that will be helpful:\n",
    "\n",
    "- `grad_check()` (run a gradient check - calls `_acc_grads` and `compute_loss`)\n",
    "- `train_sgd()` (run SGD training; more on this later)\n",
    "\n",
    "Your task is to implement the window model in `nerwindow.py`; a scaffold has been provided for you with instructions on what to fill in.\n",
    "\n",
    "When ready, you can test below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/db2 error norm = 3.555e-10 [ok]\n",
      "    b2 dims: [5] = 5 elem\n",
      "grad_check: dJ/dU error norm = 1.219e-11 [ok]\n",
      "    U dims: [5, 100] = 500 elem\n",
      "grad_check: dJ/db1 error norm = 3.701e-09 [ok]\n",
      "    b1 dims: [100] = 100 elem\n",
      "grad_check: dJ/dW error norm = 7.007e-11 [ok]\n",
      "    W dims: [100, 150] = 15000 elem\n",
      "grad_check: dJ/dL[30] error norm = 3.75e-11 [ok]\n",
      "    L[30] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[6659] error norm = 4.441e-11 [ok]\n",
      "    L[6659] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[12637] error norm = 4.31e-11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:  150\n",
      "hidden size: 100\n",
      "output size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [ok]\n",
      "    L[12637] dims: [50] = 50 elem\n"
     ]
    }
   ],
   "source": [
    "from nerwindow import WindowMLP\n",
    "clf = WindowMLP(wv, windowsize=windowsize, dims=[None, 100, 5],\n",
    "                reg=0.001, alpha=0.01)\n",
    "clf.grad_check(X_train[0], y_train[0],verbose=False) # gradient check on single point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train your model on some data! You can implement your own SGD method, but we recommend that you just call `clf.train_sgd`. This takes the following arguments:\n",
    "\n",
    "- `X`, `y` : training data\n",
    "- `idxiter`: iterable (list or generator) that gives index (row of X) of training examples in the order they should be visited by SGD\n",
    "- `printevery`: int, prints progress after this many examples\n",
    "- `costevery`: int, computes mean loss after this many examples. This is a costly operation, so don't make this too frequent!\n",
    "\n",
    "The implementation we give you supports minibatch learning; if `idxiter` is a list-of-lists (or yields lists), then gradients will be computed for all indices in a minibatch before modifying the parameters (this is why we have you write `_acc_grad` instead of applying them directly!).\n",
    "\n",
    "Before training, you should generate a training schedule to pass as `idxiter`. If you know how to use Python generators, we recommend those; otherwise, just make a static list. Make the following in the cell below:\n",
    "\n",
    "- An \"epoch\" schedule that just iterates through the training set, in order, `nepoch` times.\n",
    "- A random schedule of `N` examples sampled with replacement from the training set.\n",
    "- A random schedule of `N/k` minibatches of size `k`, sampled with replacement from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "nepoch = 5\n",
    "N = nepoch * len(y_train)\n",
    "k = 5 # minibatch size\n",
    "\n",
    "random.seed(10) # do not change this!\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "#I based myself on @msushkov code for this part, it was not clear to me until doing\n",
    "\n",
    "idxs = range(len(X_train))\n",
    "\n",
    "epochiter = itertools.repeat(idxs,3)\n",
    "trainiter = random.choice( idxs, N )\n",
    "             \n",
    "def batchesiter():\n",
    "    for i in xrange(N/k):\n",
    "        yield random.choice(idxs, k)\n",
    "\n",
    "#test mentioned below\n",
    "#clf.train_sgd(X=X_train[:100000], y=y_train[:100000], idxiter=xrange(100000), printevery=10000, costevery=10000)\n",
    "              \n",
    "#### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call `train_sgd` to train on `X_train`, `y_train`. To verify that things work, train on 100,000 examples or so to start (with any of the above schedules). This shouldn't take more than a couple minutes, and you should get a mean cross-entropy loss around 0.4.\n",
    "\n",
    "Now, if this works well, it's time for production! You have three tasks here:\n",
    "\n",
    "1. Train a good model\n",
    "2. Plot a learning curve (cost vs. # of iterations)\n",
    "3. Use your best model to predict the test set\n",
    "\n",
    "You should train on the `train` data and evaluate performance on the `dev` set. The `test` data we provided has only dummy labels (everything is `O`); we'll compare your predictions to the true labels at grading time. \n",
    "\n",
    "Scroll down to section (f) for the evaluation code.\n",
    "\n",
    "We don't expect you to spend too much time doing an exhaustive search here; the default parameters should work well, although you can certainly do better. Try to achieve an F1 score of at least 76% on the dev set, as reported by `eval_performance`.\n",
    "\n",
    "Feel free to create new cells and write new code here, including new functions (helpers and otherwise) in `nerwindow.py`. When you have a good model, follow the instructions below to make predictions on the test set.\n",
    "\n",
    "A strong model may require 10-20 passes (or equivalent number of random samples) through the training set and could take 20 minutes or more to train - but it's also possible to be much, much faster!\n",
    "\n",
    "Things you may want to tune:\n",
    "- `alpha` (including using an \"annealing\" schedule to decrease the learning rate over time)\n",
    "- training schedule and minibatch size\n",
    "- regularization strength\n",
    "- hidden layer dimension\n",
    "- width of context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:  150\n",
      "hidden size: 100\n",
      "output size: 5\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.01 s\n",
      "  [0]: mean loss 1.60944\n",
      "  [5000]: mean loss 0.391002\n",
      "  [10000]: mean loss 0.297942\n",
      "  [15000]: mean loss 0.367657\n",
      "  [20000]: mean loss 0.221587\n",
      "  [25000]: mean loss 0.181\n",
      "  [30000]: mean loss 0.156291\n",
      "  [35000]: mean loss 0.143552\n",
      "  [40000]: mean loss 0.144886\n",
      "  [45000]: mean loss 0.126348\n",
      "  Seen 50000 in 1737.07 s\n",
      "  [50000]: mean loss 0.136633\n",
      "  [55000]: mean loss 0.116553\n",
      "  [60000]: mean loss 0.115769\n",
      "  [65000]: mean loss 0.109924\n",
      "  [70000]: mean loss 0.106498\n",
      "  [75000]: mean loss 0.104109\n",
      "  [80000]: mean loss 0.103752\n",
      "  [85000]: mean loss 0.101263\n",
      "  [90000]: mean loss 0.0989455\n",
      "  [95000]: mean loss 0.0981929\n",
      "  Seen 100000 in 3427.42 s\n",
      "  [100000]: mean loss 0.0975251\n",
      "  [105000]: mean loss 0.0956838\n",
      "  [110000]: mean loss 0.0951176\n",
      "  [115000]: mean loss 0.0948409\n",
      "  [120000]: mean loss 0.0939297\n",
      "  [125000]: mean loss 0.0946812\n",
      "  [130000]: mean loss 0.0933396\n",
      "  [135000]: mean loss 0.0930836\n",
      "  [140000]: mean loss 0.0929477\n",
      "  [145000]: mean loss 0.092596\n",
      "  Seen 150000 in 5115.81 s\n",
      "  [150000]: mean loss 0.0928117\n",
      "  [155000]: mean loss 0.0922775\n",
      "  [160000]: mean loss 0.092277\n",
      "  [165000]: mean loss 0.0921667\n",
      "  [170000]: mean loss 0.0920437\n",
      "  [175000]: mean loss 0.0920323\n",
      "  [180000]: mean loss 0.091897\n",
      "  [185000]: mean loss 0.0918553\n",
      "  [190000]: mean loss 0.0918391\n",
      "  [195000]: mean loss 0.0917815\n",
      "  Seen 200000 in 6788.59 s\n",
      "  [200000]: mean loss 0.0917525\n",
      "  [203621]: mean loss 0.0917365\n",
      "SGD complete: 203621 examples in 6917.09 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  87.70%\n",
      "Mean recall:     74.76%\n",
      "Mean F1:         80.64%\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Sandbox: build a good model by tuning hyperparameters\n",
    "\n",
    "#minibatch w/ alpha schedule\n",
    "from nerwindow import eval_performance\n",
    "\n",
    "alphaiter = []\n",
    "alpha = 0.02\n",
    "for i in xrange(N/k):\n",
    "    alphaiter.append(alpha)\n",
    "    if i % 20000 == 0:\n",
    "        alpha *= 0.5          \n",
    "        \n",
    "clf = WindowMLP(wv, windowsize=windowsize, dims=[None, 100, 5], reg=0.0001, alpha=0.01)\n",
    "traincurvebest = clf.train_sgd(X=X_train, y=y_train, idxiter = batchesiter() , alphaiter= alphaiter , printevery=50000, costevery=5000)\n",
    "y_pred = clf.predict(X_dev)\n",
    "eval_performance(y_dev,y_pred,tagnames)\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom nerwindow import eval_performance\\n\\nalphaiter = []\\nalpha = 0.02\\nfor i in xrange(N/k):\\n    alphaiter.append(alpha)\\n    if i % 20000 == 0:\\n        alpha *= 0.5          \\n        \\nclf = WindowMLP(wv, windowsize=windowsize, dims=[None, 150, 5], reg=0.001, alpha=0.01)\\ntraincurvebest = clf.train_sgd(X=X_train, y=y_train, idxiter = batchesiter() , alphaiter= alphaiter , printevery=50000, costevery=5000)\\ny_pred = clf.predict(X_dev)\\neval_performance(y_dev,y_pred,tagnames)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Sandbox: build a good model by tuning hyperparameters\n",
    "\n",
    "#minibatch w/ alpha schedule\n",
    "\"\"\"\n",
    "from nerwindow import eval_performance\n",
    "\n",
    "alphaiter = []\n",
    "alpha = 0.02\n",
    "for i in xrange(N/k):\n",
    "    alphaiter.append(alpha)\n",
    "    if i % 20000 == 0:\n",
    "        alpha *= 0.5          \n",
    "        \n",
    "clf = WindowMLP(wv, windowsize=windowsize, dims=[None, 150, 5], reg=0.001, alpha=0.01)\n",
    "traincurvebest = clf.train_sgd(X=X_train, y=y_train, idxiter = batchesiter() , alphaiter= alphaiter , printevery=50000, costevery=5000)\n",
    "y_pred = clf.predict(X_dev)\n",
    "eval_performance(y_dev,y_pred,tagnames)\n",
    "\"\"\"\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e): Plot Learning Curves\n",
    "The `train_sgd` function returns a list of points `(counter, cost)` giving the mean loss after that number of SGD iterations.\n",
    "\n",
    "If the model is taking too long you can cut it off by going to *Kernel->Interrupt* in the IPython menu; `train_sgd` will return the training curve so-far, and you can restart without losing your training progress.\n",
    "\n",
    "Make two plots:\n",
    "\n",
    "- Learning curve using `reg = 0.001`, and comparing the effect of changing the learning rate: run with `alpha = 0.01` and `alpha = 0.1`. Use minibatches of size 5, and train for 10,000 minibatches with `costevery=200`. Be sure to scale up your counts (x-axis) to reflect the batch size. What happens if the model tries to learn too fast? Explain why this occurs, based on the relation of SGD to the true objective.\n",
    "\n",
    "- Learning curve for your best model (print the hyperparameters in the title), as trained using your best schedule. Set `costevery` so that you get at least 100 points to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGICAYAAAC5qRGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xm4HFWd//H3JwkBAdkJYQ9BtoDsS8KiyC4qKjoCQwQR\n",
       "XMARhRlRVEDFnwgzgqAOboDIEsVRQBAEEWQYQsIqYAJhSYIkbAlLEgIkJPf7++PUJZ2meu+b3j6v\n",
       "56mn01Wnqk6fe3Pr22dVRGBmZmbW6Qa1OgNmZmZmzeCgxszMzLqCgxozMzPrCg5qzMzMrCs4qDEz\n",
       "M7Ou4KDGzMzMuoKDGjMzM+sKDmrMzMysKzioMTMzs67goMbMzMy6goMaMzMz6woOaszM2oik5ST9\n",
       "RtL/SFqu1fkx6yQOaqxhkj4lqU/Sjq3OS60K8r5RC/OwraRLJE2V9LqkeZLuk/RtScNala9GSfqW\n",
       "pIdacN+VJf1Q0sysPB+QdFizz8/SnSPpZkmzst+jMxrNf0S8CXwa2B+oOt/VWsblU9V9qklbS3ln\n",
       "/69nSlqp2s9l3cFBjfW664HRwHOtuLmkzwD3ATsB5wAHAh8BfgeMBX7ainw1StLGwFeAU1tw+z8A\n",
       "RwHfAg4C7gHGSTqiyeevBXwGWA64OtsXjWS8X0S8BvwJ+FgzrldkWZVPLfepJm0t5X0p8DLwzSo/\n",
       "k3WLiPDmraEN+BTQB+zY4nys2OqyqDG/Y4BFpIfXkJzjQ4APdGLZABcAj7egTA/OfhcPK9p/EzAD\n",
       "GDQQ5wNrZued3sTP8nHgtWb+7JZV+dRyn3ryVE15AycA84BVlvXvobfWba6psWVG0maSrpT0vKQ3\n",
       "JE2WdEJRmndlTTGPSZovaYakP0rapijdt7Lq5x0k/V7SS8DjRcdGSRon6RVJz0m6WNIqRddZqvmp\n",
       "lnOz9B+W9FD2eZ6UdGL/Naookq8Di4HPRsSi4oMRsSgi/pTd51eSpuXc/233KlE2T0g6JNu/b851\n",
       "Pp8de3fBvoo/rzySViQFupdVLoKm+yjpQfa7ov2XAOsBuw3Q+aotm1W5MXv9QBOvOdDls2sd96kn\n",
       "T9WU9zhgBeCTVaS1LuGgxpYJSaNIVcqjgJNJf6j/BFwg6fSCpOsBL5Ie+AeRvm0tAiZK2jzn0n8A\n",
       "HiV9q/180bHfZ8cOBb4PHAGcV2WWK54r6aDs/rOATwCnZOmOokIzhKTBwD7AfRExs8o8lbpmqf2F\n",
       "ZfM5Unm/QAo4ih2T5eXhLH/V/rzy7AWsDPwt72DWN+K7kk6X9D1JP5O0fHZsxYJ0Q6rZii6/DfBI\n",
       "RBQHlQ9nr1tXyHuj5zdNRMwHbqZEE1Sbls82VaYrvM+AlHlEvAw8SHODQmtzxb/wZgPlXGAOsGdE\n",
       "vJrt+2v2MPuapAsi4pWI+F/gf/tPyh7+NwL/ID2Y/73our+KiG+XuOcvI+IH2b9vlfQuUgfMY6vI\n",
       "bzXnfgd4Gjiwv6ZF0p+Bp6q4/lrAO4C31b6UUerbaan9bysbSZcDx0s6ISLmZfu2AnYB/q0gaVU/\n",
       "rxL37f+2/uDbMppqu24DvhMR12b7fgZ8l9QH58vA9yTtDdxa4vrF1xwREf/M3q4JPJGT7KWC4+U0\n",
       "en6zXQucL2loRCzs39kB5VPLfQayzB8i9VGzHuGaGhtwklYA9iV17nuj6FvkjaQq4tFZ2iGSvp41\n",